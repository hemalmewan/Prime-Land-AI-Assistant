{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4b43cf",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e700cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import json\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf191f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform == 'win32':\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97e425",
   "metadata": {},
   "source": [
    "## **Initialize the Project Directory and Other Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c993958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM provider:openrouter\n",
      "Project Root Directory:c:\\UOC pdf\\AI Engineering Bootcamp\\mini-project-03\n"
     ]
    }
   ],
   "source": [
    "## Add project root to system path\n",
    "CURRENT_DIR=Path.cwd()\n",
    "PROJECT_ROOT=CURRENT_DIR\n",
    "\n",
    "while not (PROJECT_ROOT/'src').exists() and PROJECT_ROOT!=PROJECT_ROOT.parent:\n",
    "    PROJECT_ROOT=PROJECT_ROOT.parent \n",
    "\n",
    "if not (PROJECT_ROOT/'src').exists():\n",
    "    raise Exception(\"Could not find the 'src' folder.Check the folder structure\")\n",
    "\n",
    "## add project root to the python path\n",
    "sys.path.insert(0,str(PROJECT_ROOT))\n",
    "\n",
    "load_dotenv(PROJECT_ROOT/'.env')\n",
    "\n",
    "## get the API keys from LLM providers\n",
    "openrouter_api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "gemini_api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "\n",
    "## validate API keys\n",
    "if not openrouter_api_key and not openai_api_key and not gemini_api_key:\n",
    "    raise(\n",
    "        \"No API keys found\"\n",
    "        \"Add API keys into the .env file\"\n",
    "    )\n",
    "\n",
    "provider=\"openrouter\" if openrouter_api_key else openai_api_key\n",
    "\n",
    "print(f\"LLM provider:{provider}\")\n",
    "print(f\"Project Root Directory:{PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b33b3e",
   "metadata": {},
   "source": [
    "## **Install Dependencise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098cd42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in c:\\users\\asus\\anaconda3\\envs\\pyfix\\lib\\site-packages (from -r ../requirements.txt (line 7)) (1.58.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\asus\\anaconda3\\envs\\pyfix\\lib\\site-packages (from -r ../requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\anaconda3\\envs\\pyfix\\lib\\site-packages (from -r ../requirements.txt (line 9)) (4.13.5)\n",
      "Requirement already satisfied: aiofiles in c:\\users\\asus\\anaconda3\\envs\\pyfix\\lib\\site-packages (from -r ../requirements.txt (line 10)) (25.1.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in c:\\users\\asus\\anaconda3\\envs\\pyfix\\lib\\site-packages (from playwright->-r ../requirements.txt (line 7)) (13.0.1)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in c:\\users\\asus\\anaconda3\\envs\\pyfix\\lib\\site-packages (from playwright->-r ../requirements.txt (line 7)) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\asus\\anaconda3\\envs\\pyfix\\lib\\site-packages (from pyee<14,>=13->playwright->-r ../requirements.txt (line 7)) (4.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus\\anaconda3\\envs\\pyfix\\lib\\site-packages (from beautifulsoup4->-r ../requirements.txt (line 9)) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install -r ../requirements.txt\n",
    "!{sys.executable} -m playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd32c38",
   "metadata": {},
   "source": [
    "## **Setup Web Crawler Directries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f28b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory not exists:c:\\UOC pdf\\AI Engineering Bootcamp\\config\\config.yaml\n",
      "Directory not exists:c:\\UOC pdf\\AI Engineering Bootcamp\\config\\llm_models.yaml\n",
      "============================================================\n",
      "Pirinting All the Configuation(Non-Secrets)\n",
      "============================================================\n",
      "PROVIDER\n",
      "LLM PROVIDER:openrouter\n",
      "LLM MODEL TIER:general\n",
      "LLM CHAT MODEL:openai/gpt-4o-mini\n",
      "EMBEDDING MODEL:openai/text-embedding-3-large\n",
      "============================================================\n",
      "DIRECTRIES\n",
      "DATA DIRECTORTY:data\n",
      "VECTOR DB STORE DIRECTORY:data/vectorstore\n",
      "WEB CRAWLING OUTPUT DIRECTORY:data/processed\n",
      "MARKDWON DIRECTORY:data/markdown\n",
      "============================================================\n",
      "=========================DIRECTRIES FOR WEB CRAWLING======================\n",
      "WEB CRAWLING OUTPUT DIR:data/processed\n",
      "WEB CRAWLING MARKDOWN DIR:data/markdown\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "## Import configuration \n",
    "from src.context_engineering.config import (\n",
    "    MARKDOWN_DIR,\n",
    "    CRAWL_OUT_DIR,\n",
    "    BASE_URL,\n",
    "    MAX_DEPTH,\n",
    "    MAX_PAGES,\n",
    "    TIMEOUT,\n",
    "    RATE_LIMIT_SECONDS,\n",
    "    show_confiurations\n",
    ")\n",
    "\n",
    "##print all the configurations\n",
    "show_confiurations()\n",
    "\n",
    "## directries for web crawling\n",
    "print(\"=========================DIRECTRIES FOR WEB CRAWLING======================\")\n",
    "print(f\"WEB CRAWLING OUTPUT DIR:{CRAWL_OUT_DIR}\")\n",
    "print(F\"WEB CRAWLING MARKDOWN DIR:{MARKDOWN_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd3bab6",
   "metadata": {},
   "source": [
    "## **Import Crawler Service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7baa8ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PrimeLandsCrawler loaded from service layer\n",
      "üìç Location: src.context_engineering.crawler.primelands_crawler.PrimeLandsCrawler\n"
     ]
    }
   ],
   "source": [
    "## Import crawl web service for Prime Lands url\n",
    "from src.context_engineering.crawler.primelands_crawler import PrimeLandsCrawler\n",
    "\n",
    "\n",
    "print(\"‚úÖ PrimeLandsCrawler loaded from service layer\")\n",
    "print(\"üìç Location: src.context_engineering.crawler.primelands_crawler.PrimeLandsCrawler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea2bee",
   "metadata": {},
   "source": [
    "## **Load Crawl Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f941c7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Crawl Configurations....\n",
      "============================================================\n",
      "Base URL:5\n",
      "Max Depth:3\n",
      "Max pages:20\n",
      "Timeout Limit:30000\n",
      "Rate Limit Seconds:2.0\n"
     ]
    }
   ],
   "source": [
    "## crawl configurations\n",
    "BASE_URL=\"https://www.primelands.lk\"\n",
    "\n",
    "START_URL = [\n",
    "    \"/land/en\",\n",
    "    \"/house/en\",\n",
    "    \"/apartment/ongoing/en\",\n",
    "    \"/apartment/completed/en\",\n",
    "    \"/portfolio-property/en\"\n",
    "]\n",
    "\n",
    "START_URL=[BASE_URL+path for path in START_URL]\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Crawl Configurations....\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base URL:{len(START_URL)}\")\n",
    "print(f\"Max Depth:{MAX_DEPTH}\")\n",
    "print(f\"Max pages:{MAX_PAGES}\")\n",
    "print(f\"Timeout Limit:{TIMEOUT}\")\n",
    "print(f\"Rate Limit Seconds:{RATE_LIMIT_SECONDS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a170d2b",
   "metadata": {},
   "source": [
    "## **Execute Crawl Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2778f3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting crawl at 12:14:39\n",
      "\n",
      "[0] Crawling:https://www.primelands.lk/land/en\n",
      " Saved (2597 chars)\n",
      "[0] Crawling:https://www.primelands.lk/house/en\n",
      " Saved (2599 chars)\n",
      "[0] Crawling:https://www.primelands.lk/apartment/ongoing/en\n",
      " Saved (2631 chars)\n",
      "[0] Crawling:https://www.primelands.lk/apartment/completed/en\n",
      " Saved (603 chars)\n",
      "[0] Crawling:https://www.primelands.lk/portfolio-property/en\n",
      " Saved (2618 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/LUXORA-KIRIBATHGODA/en\n",
      "Error:Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "  - navigating to \"https://www.primelands.lk/land/LUX\n",
      "[1] Crawling:https://www.primelands.lk/land/district/Ratnapura/en\n",
      " Saved (2635 chars)\n",
      "[1] Crawling:https://www.primelands.lk/agriculture-land/en\n",
      " Saved (2614 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/THALAHENA-LUXE/en\n",
      " Saved (2631 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/NEXUS-TOWN-ALUTHGAMA/en\n",
      "Error:Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "  - navigating to \"https://www.primelands.lk/land/NEX\n",
      "[1] Crawling:https://www.primelands.lk/close-to-Airport/en\n",
      " Saved (2614 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/city/Nuwara-Eliya/en\n",
      " Saved (2633 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/AURELIA-ALUBOMULLA/en\n",
      " Saved (2639 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/district/Kegalle/en\n",
      " Saved (2631 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/city/Moratuwa/en\n",
      " Saved (2625 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/city/Boralesgamuwa/en\n",
      " Saved (2635 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/city/Gampaha/en\n",
      " Saved (2623 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/city/Galle/en\n",
      " Saved (2619 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/LUXCORE-CITY-MALABE/en\n",
      " Saved (2641 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/city/Panadura/en\n",
      " Saved (2625 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/district/Matale/en\n",
      " Saved (2629 chars)\n",
      "[1] Crawling:https://www.primelands.lk/land/district/Hambantota/en\n",
      " Saved (2637 chars)\n",
      "\n",
      "‚úÖ Crawl complete in 199.3s\n",
      "üìÑ Documents collected: 20\n",
      "üîó URLs visited: 22\n"
     ]
    }
   ],
   "source": [
    "## start time initialized\n",
    "start_time=time.time()\n",
    "\n",
    "## call the \"PrimeLandsCrawler\" class and create an object\n",
    "land_crawler=PrimeLandsCrawler(\n",
    "    base_url=BASE_URL,\n",
    "    max_depth=MAX_DEPTH,\n",
    "    max_pages=MAX_PAGES,\n",
    "    timeout=TIMEOUT,\n",
    "    rate_limit_seconds=RATE_LIMIT_SECONDS\n",
    "    )\n",
    "\n",
    "## execute crawler\n",
    "print(f\"\\nüöÄ Starting crawl at {time.strftime('%H:%M:%S')}\\n\")\n",
    "documents= land_crawler.crawl(START_URL)\n",
    "\n",
    "elapsed=time.time() -start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Crawl complete in {elapsed:.1f}s\")\n",
    "print(f\"üìÑ Documents collected: {len(documents)}\")\n",
    "print(f\"üîó URLs visited: {len(land_crawler.visited)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d1619",
   "metadata": {},
   "source": [
    "## **Save Output Files(JSONL and Markdown Format)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e467127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Prime Lands Corpus to c:\\UOC pdf\\AI Engineering Bootcamp\\mini-project-03\\data\\processed\\prime_lands_corpus.jsonl\n"
     ]
    }
   ],
   "source": [
    "## save output as a JASONL format\n",
    "name_of_file=\"prime_lands_corpus.jsonl\"\n",
    "JSON_PATH=PROJECT_ROOT/CRAWL_OUT_DIR/name_of_file\n",
    "with open(JSON_PATH,\"w\",encoding=\"UTF-8\") as f:\n",
    "    for doc in documents:\n",
    "        f.write(json.dumps(doc,ensure_ascii=False)+'\\n')\n",
    "print(f\"Save Prime Lands Corpus to {JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec94da6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved Markdown files to c:\\UOC pdf\\AI Engineering Bootcamp\\mini-project-03\\data\\markdown\n"
     ]
    }
   ],
   "source": [
    "## save output as Markdown format\n",
    "for doc in documents:\n",
    "    property_id = doc.get(\"property_id\") or \"page\"\n",
    "    filename = re.sub(r'[^a-zA-Z0-9_-]', '_', str(property_id))\n",
    "    filepath = PROJECT_ROOT/MARKDOWN_DIR / f\"{filename}.md\"\n",
    "\n",
    "    markdown_content = f\"\"\"# {doc.get('title', 'No Title')}\n",
    "\n",
    "URL: {doc.get('url')}\n",
    "Headings: {doc.get('headings')}\n",
    "Price: {doc.get('price')}\n",
    "Bedrooms: {doc.get('bedrooms')}\n",
    "Bathrooms: {doc.get('bathrooms')}\n",
    "Size (sqft): {doc.get('sqft')}\n",
    "Agent: {doc.get('agent')}\n",
    "\n",
    "---\n",
    "\n",
    "## Content\n",
    "\n",
    "{doc.get('content')}\n",
    "\"\"\"\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(markdown_content)\n",
    "\n",
    "print(f\"‚úÖ Saved Markdown files to {PROJECT_ROOT/MARKDOWN_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae5795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
